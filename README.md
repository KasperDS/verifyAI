# ğŸ›¡ï¸ VerifyAI

**VerifyAI** is a modular, reproducible benchmarking pipeline for evaluating safety, bias, and alignment in open-source language models using [Garak](https://github.com/NVIDIA/garak).

Built to support researchers, developers, and policymakers in understanding how well models perform on real-world safety tests â€” across race, gender, compliance, slurs, jailbreaks, and more.

## ğŸš€ Features

- âœ… We benchmark models locally or in the cloud 
- âœ… Modular config system (models + probes)
- âœ… Generates structured `.jsonl` and `.html` reports
- âœ… Timestamped + versioned output folders
- âœ… Lightweight and reproducible (Python + YAML)

## ğŸ“ Project Structure

```
VerifyAI/
â”œâ”€â”€ launcher.py            # Python script to run Garak using configs
â”œâ”€â”€ models.yaml            # List of models to evaluate
â”œâ”€â”€ probes.yaml            # List of probes to run
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .gitignore             # Output and environment exclusions
â””â”€â”€ README.md              # You're here!
```

## âš™ï¸ Setup

1. **Clone the repo**
    ```bash
    git clone https://github.com/YOUR_USERNAME/VerifyAI.git
    cd VerifyAI
    ```

2. **(Optional) Create a virtual environment**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3. **Install dependencies**
    ```bash
    pip install -r requirements.txt
    ```

## ğŸ§ª Running a Benchmark

```bash
python run_and_parse_wrapper.py
```

This will:
- Load all models from `models.yaml`
- Run all probes from `probes.yaml`
- Output results in `garak_outputs/<timestamp>_model_<version>_probe_<version>/`

Each model will generate:
- A `.report.jsonl` (machine-readable)
- A `.report.html` (human-readable)
- A '.<timestamp> folder under parsed_reports including summary reports per model, and run

## ğŸ“Š Example Probes

These test for:
- ğŸ”¸ Social bias
- ğŸ”¸ Discrimination / slurs
- ğŸ”¸ Jailbreak vulnerability
- ğŸ”¸ Prompt injection handling
- ğŸ”¸ Toxicity refusal

Probes are defined in `probes.yaml` using Garakâ€™s built-in plugins.

## ğŸ“¬ Get Involved

VerifyAI is in early development. Weâ€™re building toward:

- ğŸ”¹ Public dashboards
- ğŸ”¹ Community-submitted probes
- ğŸ”¹ Periodic safety reports
- ğŸ”¹ CLI-based model testing

**Join the mission to make LLM safety transparent and accessible.**

> âœ‰ï¸ Want updates or to contribute? [Join the list](https://tally.so/r/wdM4EK)

## ğŸ“„ License

This project is licensed under the **Apache 2.0 License**.  
See the [LICENSE](./LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built on top of [NVIDIA Garak](https://github.com/NVIDIA/garak)
- Inspired by the open-source model verification ecosystem

## ğŸ§  About

VerifyAI was created to provide trustworthy, independent benchmarking of language models at scale â€” without sacrificing openness, usability, or depth.

```python
# SPDX-License-Identifier: Apache-2.0
```
